# -*- coding: utf-8 -*-
"""Project_DM_Multiclass.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11pnM5nG43GeNUGYoJMCoIMIefoPEOLU7

> HCV Project: Final Draft- Multiclass
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt 
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn import tree
from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score
from sklearn.metrics import confusion_matrix as cm

from sklearn.model_selection import cross_val_score


# %matplotlib inline

from yellowbrick.classifier import ClassificationReport,ConfusionMatrix,ROCAUC,ClassPredictionError,ClassBalance 
from yellowbrick.model_selection import ValidationCurve,CVScores
from sklearn.neighbors import KNeighborsClassifier

import pydotplus 

from statistics import mean
from sklearn.model_selection import KFold

from IPython.display import Image
from yellowbrick.classifier import ClassificationReport,ConfusionMatrix

from sklearn.neighbors import KNeighborsClassifier # wtf 

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

from sklearn.decomposition import PCA

import matplotlib.gridspec as gridspec

#import tensorflow as tf

# read data 
df = pd.read_csv('hcvdat0.csv')

df.head()

df.info()

def visualizer_fit_score_show(visualizer,X_train,y_train,X_test,y_test):
  visualizer.fit(X_train, y_train)  
  visualizer.score(X_test, y_test)
  visualizer.poof()

size = (800,500)
def visualize_all(classifier,X_train,y_train,X_test,y_test,y):

    ClassBalance(size=size).fit(y.values.ravel()).poof()
    ClassBalance(size=size,title='Training : Class Balances for '+str(len(y_train))+" instances").fit(y_train.values.ravel()).poof()
    ClassBalance(size=size,title='Testing : Class Balances for '+str(len(y_test))+" instances").fit(y_test.values.ravel()).poof()
    visualizer_fit_score_show(ConfusionMatrix(classifier,size=size),X_train,y_train,X_test,y_test)
    visualizer_fit_score_show(ClassificationReport(classifier,size=size),X_train,y_train,X_test,y_test)
    visualizer_fit_score_show(ClassPredictionError(classifier,size=size),X_train,y_train.values.ravel(),X_test,y_test.values.ravel())

"""### We wil detect and work on null values """

# check for na in entire df 
df.isnull().values.any()

# check for no of na entry in total column 
df.isnull().sum()

# ALB  1 
# ALP  18 DONE 
# ALT  1 
# CHOL 10 DONE
# PROT 1

# check no of null entry in ALP column 
df['ALP'].isnull().sum()

"""### What I want to do: 
### 1. Find the mean in attribute column for a given target label 
### 2. Replace the mean in the columns of that attribute column corresponding to a given target value 
"""

alp = df['ALP']
alp

# find the mean of ALP 
alp.mean(skipna=True)

target= df['Category']
#alp.join( target)

#pd.merge(alp, target) # no common columns to merge on

alp = pd.concat( [alp, target] , axis=1) # concatenated successfully 
alp.head()

# now, ALP and its target label are

y = df['Category']

# see unique values in target column 

df['Category'].unique()

alp.isnull().sum()

alp0 = alp[ alp['Category'] == '0=Blood Donor'] # Sanuj recommended this step on Assignment 1 
alp0.head() # Target: 0 Blood Donor

alp0.mean()

# check if any null value in alp0 
alp0.isnull().sum() # no null

alp0s = alp[ alp['Category'] == '0s=suspect Blood Donor'] # Sanuj recommended this step on Assignment 1 
alp0s.isnull().sum() # no null
alp0s

alp0s.mean()

alp1 = alp[ alp['Category'] == '1=Hepatitis'] # Sanuj recommended this step on Assignment 1 
alp1.isnull().sum() # 3 null in 18 instances 
#alp1 # to see null values

# we fill null values with the mean of the column 
#alp1.fillna(alp1['ALP'].mean(skipna=True), inplace = True)

alp1['ALP'].mean(skipna=True) # 42.11

#alp1['ALP'] = alp1['ALP'].fillna(mu)

#df.loc[np.isnan(df["Age"]), 'Age'] = rand1
#alp1.loc[np.isnan(df["ALP"]), 'ALP'] = mu


# fillna is throwing error 

alp1_ = alp1.fillna(alp1['ALP'].mean(skipna=True)) # throwing error when we add inplace=True
alp1_.head()

# do the steps above for ALP 2 


# 2=Fibrosis

alp2 = alp[ alp['Category'] == '2=Fibrosis'] # Sanuj recommended this step on Assignment 1 
alp2.isnull().sum() #  9 null in 21 instances 

alp2.shape

alp2_ = alp2.fillna(alp2['ALP'].mean(skipna=True)) # throwing error when we add inplace=True
alp2_.isnull().sum()

#3=Cirrhosis

alp3 = alp[ alp['Category'] == '3=Cirrhosis'] # Sanuj recommended this step on Assignment 1 
alp3.isnull().sum() #  6 null in 30 instances 

#alp3.shape

alp3_ = alp3.fillna(alp3['ALP'].mean(skipna=True)) # throwing error when we add inplace=True
alp3_.isnull().sum()

"""### concat alp0, alp0s, alp1_, alp2_, alp3_ onto a single df of 2 columns (Category, ALP) """

alp0
alp0s
alp1_ 
alp2_ 
alp3_ 

frames = [alp0, alp0s, alp1_ , alp2_ ,alp3_ ]
new_alp = pd.concat(frames)

new_alp.shape

new_alp.isnull().sum()

"""### Work on CHOL column """

df['CHOL'].isnull().sum() # there are 10 null values in CHOL column

df['CHOL'].min()

df['CHOL'].max()

df['CHOL'].median()

df['CHOL'].mean()

"""### FINDING: We see that the median and mean for CHOL is very close"""

chol = df['CHOL']


chol = pd.concat( [chol, target] , axis=1) # concatenated successfully 
chol.head()

"""### For each target label, find the mean of CHOL """

chol0 = chol[ chol['Category'] == '0=Blood Donor'] # Sanuj recommended this step on Assignment 1 
chol0.isnull().sum() #  7 null in 533 instances 

chol0.shape

print('chol0 min', chol0['CHOL'].min())
print('chol0 max', chol0['CHOL'].max())
print('chol0 mean: ', chol0['CHOL'].mean())
print('chol0 median: ', chol0['CHOL'].median())

# df[df['A'].isnull()].index.tolist()
chol0[chol0['CHOL'].isnull()].index.tolist()

# fill null values with mean 
chol0['CHOL']
chol0_ = chol0.fillna(chol0['CHOL'].mean(skipna=True)) # throwing error when we add inplace=True

chol0_.isnull().sum() # no null

chol['Category'].unique() # unique values in Category column

chol0s = chol[ chol['Category'] == '0s=suspect Blood Donor'] # Sanuj recommended this step on Assignment 1 
chol0s.isnull().sum() #  0 null in 7 instances 

chol0s.shape

chol1 = chol[ chol['Category'] == '1=Hepatitis'] # Sanuj recommended this step on Assignment 1 
chol1.isnull().sum() #  0 null in 24 instances 

#chol1.shape

# '2=Fibrosis'

chol2 = chol[ chol['Category'] == '2=Fibrosis'] # Sanuj recommended this step on Assignment 1 
chol2.isnull().sum() #  1 null in 21 instances 

#chol2.shape

chol2_ = chol2.fillna(chol2['CHOL'].mean(skipna=True)) # throwing error when we add inplace=True
chol2_.isnull().sum()

# '3=Cirrhosis'

chol3 = chol[ chol['Category'] == '3=Cirrhosis'] # Sanuj recommended this step on Assignment 1 
chol3.isnull().sum() #  2 null in 30 instances 

chol3.shape

chol3_ = chol3.fillna(chol3['CHOL'].mean(skipna=True)) # throwing error when we add inplace=True
chol3_.isnull().sum()

# CHID: merge chol0, chol0s, chol1 , chol2, chol3 onto a single df 

chol0_
chol0s
chol1 
chol2_ 
chol3_ 

chol_frames = [chol0_, chol0s, chol1 , chol2_ ,chol3_ ]
new_alp1 = pd.concat(chol_frames)

new_alp1.shape

new_alp1.isnull().sum()

new_alp1.head(5)

# alt - 1 null instance 
# prot - 1 null instance 

# alb - 1 null instance 

#print(df['ALB'].isnull().sum())

# df[df['A'].isnull()].index.tolist() --> get index

df[ df['ALB'].isnull() ].index.tolist() # at 603, there is null value 
print(df.loc[603, :]) # we see target = 3

# get mean of alb at 3=Cirrhosis

alb = df['ALB']

alb = pd.concat( [ alb, target ], axis=1) #df['ALB', 'Category']
alb3 = alb[ alb['Category'] == '3=Cirrhosis']

print('ALB 3 mean ', alb3['ALB'].mean() ) 
print('ALB 3 median ', alb3['ALB'].median() )
print('ALB 3 max ', alb3['ALB'].max() )
print('ALB 3 min ', alb3['ALB'].min() )

alb3_mean = alb3['ALB'].mean();

# CHID: fill with mean at that specific index

# CHID: Please add the mean ALB3 value at index 603(row) column ALB
df['ALB'].fillna(value= alb3_mean, inplace=True)

df.isnull().sum()

# ALT 

df[ df['ALT'].isnull() ].index.tolist() # at 540, there is null value 
print(df.loc[540, :]) # we see target = 1

# get mean of alt at 1=Hepatitis

alt = df['ALT']

alt = pd.concat( [ alt, target ], axis=1) #df['ALT', 'Category']
alt1 = alt[ alt['Category'] == '1=Hepatitis']
alt1_mean = alt1['ALT'].mean();

print('ALT 1 mean ', alt1['ALT'].mean() ) 
print('ALT 1 median ', alt1['ALT'].median() )
print('ALT 1 max ', alt1['ALT'].max() )
print('ALT 1 min ', alt1['ALT'].min() )

# CHID: fill with mean at that specific index
df['ALT'].fillna(value= alt1_mean, inplace=True)
df.isnull().sum()

df[ df['PROT'].isnull() ].index.tolist() # at 590, there is null value 
print(df.loc[590, :]) # we see target = 3

# get mean of PROT at 3=Cirrhosis

prot = df['PROT']

prot = pd.concat( [ prot, target ], axis=1) #df['ALT', 'Category']
prot3 = prot[ prot['Category'] == '1=Hepatitis']



prot3.head()

print('PROT 3 mean ', prot3['PROT'].mean() ) 
print('PROT 3 median ', prot3['PROT'].median() )
print('PROT 3 max ', prot3['PROT'].max() )
print('PROT 3 min ', prot3['PROT'].min() )

# CHID: fill with mean at that specific index
prot3_mean = prot3['PROT'].mean();
df['PROT'].fillna(value= prot3_mean, inplace=True)
df.isnull().sum()

#Replace the columns of ALP and CHOL with the new generated dataframes.

df = df.assign(ALP= new_alp['ALP'], CHOL= new_alp1['CHOL']);
df.isnull().sum()

# count the number of unique entries 

df.groupby('Category').nunique()

#Storing the df for multi-class classification
df_multiclass = df.copy()

"""### At this point, we convert 0/0s to 0 and 1/2/3 to 1

#### Notice that, 
#### 0 -> 533+7 = 540 instances, and
#### 1/2/3 -> 24+21+30 = 75 instances
"""

df_test = df.loc[(df.Category == '0=Blood Donor'), 'Category'] = 'No-Hepatitis'
df.groupby('Category').nunique()

df_test = df.loc[(df.Category == '0s=suspect Blood Donor'), 'Category'] = 'No-Hepatitis'
df.groupby('Category').nunique()

df.loc[(df.Category == '1=Hepatitis'), 'Category'] = 'Hepatitis'
df.loc[(df.Category == '2=Fibrosis'), 'Category'] = 'Hepatitis'


# df.groupby('Category').nunique()

df.loc[(df.Category == '3=Cirrhosis'), 'Category'] = 'Hepatitis'
df.groupby('Category').nunique()

# SR: We need to work on the gender column. Convert M and F to 1 and 0 
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
category = le.fit_transform( df['Category'] ) 



# df.loc[(df.Category == 'Hepatitis'), 'Category'] = 1
# df.loc[(df.Category == 'No-Hepatitis	'), 'Category'] = 0



#type(gender) # numpy array 

# replace 'Sex' column w/ values in gender array
print (category)
df['Category']= category

df.loc[(df.Sex == 'm'), 'Sex'] = 1
df.loc[(df.Sex == 'f'), 'Sex'] = 0

df.groupby('Category').nunique()

# Covariation and Correlation
print ('Covariance:', df.cov());   
print ('Correlation:', df.corr());

df.head(5)

df.tail() # we see there is NaN entry- we need to address this

df.head(10)

Y= pd.DataFrame(data= df['Category']);
X= df.drop(['Category'], axis=1);

X_colnames = X.columns;

# SR: oversample the data 

from imblearn.over_sampling import SMOTE


sm = SMOTE(random_state=42)
X_res, Y_res = sm.fit_resample(X, Y)
type(Y)

print (type(X_res))

X.shape # (615, 13) 51
X_res.shape # (1080, 13)

X_resDataframe = pd.DataFrame(data=X_res, columns=X_colnames);
Y_resDataframe =  pd.DataFrame(data=Y_res, columns=["Category"]);

Y_resDataframe.info()

X_resDataframe.info()

X_train, X_test, Y_train, Y_test = train_test_split(X_resDataframe,Y_resDataframe,train_size=0.8, test_size=0.2, random_state=1) 
Y_train.head(5)

"""Standardize df before oversampling: DONE




"""

# SR: standarize the dataset 
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

X_train_std = sc.fit_transform(X_train)
X_test_std = sc.transform(X_test)

"""SR: Apply PCA, algorithms, and performance metrics on standardized data. Repeat the steps on the oversampled data. """

unique, frequency = np.unique(Y_res,  
                              return_counts = True) 
# print unique values array 
print("Unique Values:",  
      unique) 
  
# print frequency array 
print("Frequency Values:", 
      frequency)

unique, frequency = np.unique(Y,  
                              return_counts = True) 
# print unique values array 
print("Unique Values:",  
      unique) 
  
# print frequency array 
print("Frequency Values:", 
      frequency)

Y_bc_resDataframe =  pd.DataFrame(data=Y_res, columns=["Category"]);
ClassBalance(size=size).fit(Y_bc_resDataframe.values.ravel()).poof()

ClassBalance(size=size,title='Training : Class Balances for '+str(len(Y_train))+" instances").fit(Y_train.values.ravel()).poof()
ClassBalance(size=size,title='Testing : Class Balances for '+str(len(Y_test))+" instances").fit(Y_test.values.ravel()).poof()

"""# Multi-class classification"""

#Multiclass classification - Starts here.
df_multiclass.head(5)

df_multiclass.groupby('Category').nunique()

#Displaying the dataframe.
# df_multiclass['Category']= category
print (df_multiclass.head(5))

"""Converting the columns sex values"""

#Converting the sex column male and female to 1's and 0's
df_multiclass.loc[(df_multiclass.Sex == 'm'), 'Sex'] = 1
df_multiclass.loc[(df_multiclass.Sex == 'f'), 'Sex'] = 0
df_multiclass.head(5)

df_multiclass.groupby('Category').nunique()

Y_multiclass= pd.DataFrame(data= df_multiclass['Category']);
X_multiclass= df_multiclass.drop(['Category'], axis=1);

X_multiclass_colnames = X_multiclass.columns;

# SR: check if all the target values are present 

df_multiclass['Category'].unique()

"""**Data oversampling to address the class imbalance problem**"""

# oversampling the data 

from imblearn.over_sampling import SMOTE


sm = SMOTE(random_state=42)
X_res_multiclass, Y_res_multiclass = sm.fit_resample(X_multiclass, Y_multiclass)
# type(Y)

# print (type(X_res))

X_res_multiclass.shape # (615, 13) 51
Y_res_multiclass.shape # (1080, 13)

X_resDataframe1 = pd.DataFrame(data=X_res_multiclass, columns=X_multiclass_colnames);
Y_resDataframe1 =  pd.DataFrame(data=Y_res_multiclass, columns=["Category"]);

"""**Resampled data- Check**"""

X_resDataframe1.shape

Y_res_multiclass.shape

X_resDataframe1= X_resDataframe1.drop(["Unnamed: 0"], axis=1);
X_resDataframe1.head(5)

Y_resDataframe1.head(5)

"""**Standardize the X_resDataframe1 dataset**



"""

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_resDataframe1_transform= sc.fit_transform(X_resDataframe1)
X_resDataframe1_df = pd.DataFrame(data=X_resDataframe1_transform, columns=X_multiclass_colnames[1:]);

"""**Checking whether the features are standardized or not**"""

X_resDataframe1_df.head(5)

"""**Checking the unique and frequency values in Y_resDataframe**"""

unique, frequency = np.unique(Y_resDataframe1,  
                              return_counts = True) 
# print unique values array 
print("Unique Values:",  
      unique) 
  
# print frequency array 
print("Frequency Values:", 
      frequency)

"""**Splitting the dataset into xtrain, ytrain, xtest and ytest**"""

#Train and test dataset for multi-label classification
X_train, X_test, Y_train, Y_test = train_test_split(X_resDataframe1_df,Y_resDataframe1,train_size=0.8, test_size=0.2, random_state=1)

"""**Standardizing the dataset**"""

# SR: standarize the dataset 
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

X_train_std = sc.fit_transform(X_train)
X_test_std = sc.transform(X_test)

"""**Decision tree for Multi-class classification starts here**"""

# SR: Alright: Goes into report 

#Classification algorithm - Decision tree
maxdepths = [*range(1,50,1)];

#To keep track of the k scores.
scores = [];
plotScores = [];

trainAcc = np.zeros(len(maxdepths))
testAcc = np.zeros(len(maxdepths))


index = 0
for depth in maxdepths:
    clf = tree.DecisionTreeClassifier(max_depth=depth, criterion="entropy")
    clf = clf.fit(X_train_std, Y_train)
    Y_predTrain = clf.predict(X_train_std)
    Y_predTest = clf.predict(X_test_std)
    trainAcc[index] = accuracy_score(Y_train, Y_predTrain)
    testAcc[index] = accuracy_score(Y_test, Y_predTest)
    scores.append([depth, accuracy_score(Y_test, Y_predTest)])
    plotScores.append(accuracy_score(Y_test, Y_predTest));  
    index+=1;
        
plt.plot(maxdepths,trainAcc,'ro-',maxdepths,testAcc,'bv--')
plt.legend(['Training Accuracy','Test Accuracy'])
plt.xlabel('Max depth')
plt.ylabel('Accuracy')
plt.title("Decision Tree-Accuracy")
print ('Mean Trainaccuracy: %0.5f'%mean(trainAcc));
print ('Mean Testaccuracy: %0.5f' %mean(testAcc));

#printing the test accuracy scores.
for i in scores:
  print ("The maxDepth value is %d and the accuracy score is %0.5f" % (i[0], i[1]));

#Finding and printing the optimum Depth value for Decision tree
finalResult = [];
maxAccuracy = max(plotScores);

for i in scores:
  if (i[1]== maxAccuracy):
   optimum_Depth = i[0]  #optimumDepth value
   finalResult.append([i[0], i[1]]);

for i in finalResult:
 print ("The optimum maxdepth value is %d and the max test accuracy score is %0.5f" % (i[0],i[1]));

"""**KNN classifier for Multi-class classification starts here**"""

#KNN classifier.
i_ = []
mu = []
for i in range(1, 101): 
    
    knn = KNeighborsClassifier(n_neighbors=i)
    #cross_val_score( X, y) # parameters: estimator, X, target label, n_splits 
    scores = cross_val_score(knn, X_resDataframe1_df, Y_resDataframe1.values.ravel(), cv=10, scoring='accuracy') 
    
    #print(scores)
    #print('Scores for i= ', i, ': ', scores.mean())
    i_.append( i ) # k value 
    mu.append( scores.mean() ) # accuracy

print()

# plot test and training accuracy onto one graph 

plt.plot(i_, mu, label='Accuracy vs K value')

plt.xlabel('K value')
plt.ylabel('Accuracy/%')

plt.legend(loc='upper right')
plt.grid(alpha=0.5)

plt.savefig('Best K Value: k_fold_cross_validation_acc_vs_kvalue.png') # We need this 

plt.show()

# Find the k value for maximum accuracy 
max(mu) # 98 %
max_ = []
# find the indices for max
for i in range( len(mu) ):
    if mu[i] == max(mu):
        max_.append(i+1)

best_k_mc = max_[0]        
print ("The max accuracy for the original dataset %0.6f" %max(mu))
print('Optimum k value for the original dataset', max_ )

"""**SVM linear algorithm for Multi-class classification starts here**"""

#Algorithm - SVM-linear

C = [0.01, 0.1, 0.2, 0.5, 0.8, 1, 5, 10, 20, 50]
SVMtrainAcc = []
SVMtestAcc = []

#To keep track of the test accuracy scores.
scores = [];
plotScores = [];

for param in C:
    clf = SVC(C=param, kernel='linear')
    clf.fit(X_train_std, Y_train.values.ravel())
    Y_predTrain = clf.predict(X_train_std)
    Y_predTest = clf.predict(X_test_std)
    SVMtrainAcc.append(accuracy_score(Y_train, Y_predTrain))
    SVMtestAcc.append(accuracy_score(Y_test, Y_predTest))
    scores.append([param, accuracy_score(Y_test, Y_predTest)])
    plotScores.append(accuracy_score(Y_test, Y_predTest));

#SVM -metrics
plt.plot(C, SVMtrainAcc, 'ro-', C, SVMtestAcc,'bv--')
plt.legend(['Training Accuracy','Test Accuracy'])
plt.xlabel('C')
plt.xscale('Log')
plt.ylabel('Accuracy')
plt.title("SVM Linear - Accuracy")
print ('Trainaccuracy: %0.5f'%mean(SVMtrainAcc));
print ('Testaccuracy: %0.5f' %mean(SVMtestAcc));

#printing the test accuracy scores.
for i in scores:
  print ("The param value is %f and the accuracy score is %0.5f" % (i[0], i[1]));

#Finding and printing the optimum param value for SVM linear model - The optimal parameter value is 10.
finalResult = [];
maxAccuracy = max(plotScores);

for i in scores:
  if (i[1]== maxAccuracy):
   optimum_param = i[1]  #optimumParam value
   finalResult.append([i[0], i[1]]);

for i in finalResult:
 print ("The optimum parameter value is %f and the max test accuracy score is %0.5f" % (i[0],i[1]));

"""**Logistic Regression for Multi-class classification starts here**

---


"""

#Algorithm - Logistic regression

from sklearn import linear_model
from sklearn.svm import SVC

C = [0.01, 0.1, 0.2, 0.5, 0.8, 1, 5, 10, 20, 50]
LRtrainAcc = []
LRtestAcc = []

#To keep track of the test accuracy scores.
scores = [];
plotScores = [];


for param in C:
    clf = linear_model.LogisticRegression(C=param)
    clf.fit(X_train_std, Y_train.values.ravel())
    Y_predTrain = clf.predict(X_train_std)
    Y_predTest = clf.predict(X_test_std)
    LRtrainAcc.append(accuracy_score(Y_train, Y_predTrain))
    LRtestAcc.append(accuracy_score(Y_test, Y_predTest))
    scores.append([param, accuracy_score(Y_test, Y_predTest)])
    plotScores.append(accuracy_score(Y_test, Y_predTest));

#Logistic regression -metrics
plt.plot(C, LRtrainAcc, 'ro-', C, LRtestAcc,'bv--')
plt.legend(['Training Accuracy','Test Accuracy'])
plt.xlabel('C')
plt.xscale('Log')
plt.ylabel('Accuracy')
plt.title("Logistic Regression - Accuracy")
print ('Mean Trainaccuracy: %0.5f'%mean(SVMtrainAcc));
print ('Mean Testaccuracy: %0.5f' %mean(SVMtestAcc));

#printing the test accuracy scores.
for i in scores:
  print ("The param value is %f and the accuracy score is %0.5f" % (i[0], i[1]));

#Finding and printing the optimum param value for Logistic regression model
finalResult = [];
maxAccuracy = max(plotScores);

for i in scores:
  if (i[1]== maxAccuracy):
   optimum_param_LR = i[1]  #optimumParam value
   finalResult.append([i[0], i[1]]);

for i in finalResult:
 print ("The optimum param value is %f and the max test accuracy score is %0.5f" % (i[0],i[1]));

"""**Ensemble learning for Multi-class classification**"""

# 7.2 Majority Voting 

# Ensemble Classifier

# We need multiple classifiers here. I choose
# 1. Logistic Regression
# 2. Decision Tree with Entropy
# 3. kNN with Euclidean Distance 
# 4. non-linear SVM 

clf1 = LogisticRegression(C=optimum_param_LR) # C = 0.2 - Check with SR- If time permits.
clf2 = SVC(C=10)  # linear SVM C = 10 
clf3 = KNeighborsClassifier(n_neighbors=best_k) # pass best_k=7
clf4 = DecisionTreeClassifier(max_depth= optimum_Depth, criterion='entropy') # - Check with SR


labels = ['Logistic Regression', 'Support Vector Machine', 'K Nearest Neighbor', 'Decision Tree', 'Ensemble']
eclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3, clf4], weights= [1, 1, 1, 1] )  # works


for clf, label in zip([clf1, clf2, clf3, clf4, eclf] , labels): 
  
  clf.fit(X_resDataframe, Y_resDataframe)
 
  scores = model_selection.cross_val_score(clf, X_resDataframe1_df, Y_resDataframe1.values.ravel(), 
                                              cv=20, # returned better accuracy
                                              scoring='accuracy')
  print("Accuracy: %0.2f (+/- %0.2f) [%s]" % (scores.mean(), scores.std(), label))

# Bagging Classifier for Multi-class classification 

# We use unpruned Decision Tree as our classifer in Bagging 

tree = DecisionTreeClassifier(criterion='entropy', 
                              random_state=1, 
                              max_depth=None) # We keep max_depth=None to have unpruned Decision Tree

# The number of trees can be set via the “n_estimators” argument 
bag = BaggingClassifier(base_estimator=tree, 
                        n_estimators= 10, 
                        bootstrap=True, 
                        bootstrap_features=False,                          
                        random_state=1,
                        oob_score=True
                        )


bag.fit(X_train_std, Y_train)

print('Out Of Bag Score: ', bag.oob_score_)
print('Test Accuracy Score: ', bag.score(X_test_std, Y_test), '\n\n\n' )

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
category = le.fit_transform( Y_resDataframe1['Category'] ) 



# df.loc[(df.Category == 'Hepatitis'), 'Category'] = 1
# df.loc[(df.Category == 'No-Hepatitis	'), 'Category'] = 0



#type(gender) # numpy array 

# replace 'Sex' column w/ values in gender array
print (category)
Y_resDataframe1['Category']= category

#Making dataset ready for the random forest and decision boundary.
Xr_train, Xr_test, yr_train, yr_test = train_test_split(X_resDataframe1_df, Y_resDataframe1, test_size=0.2)

# Random Forest 

# Xr_train, Xr_test, yr_train, yr_test

from sklearn.ensemble import RandomForestClassifier 

forest = RandomForestClassifier(criterion='gini', n_estimators=100, random_state=1)


# convert X_resDataFrame_std to numpy
Xr_train_np = pd.DataFrame(Xr_train).to_numpy()
Xr_train_np = Xr_train_np.astype(np.float)

pca = PCA(n_components=2)

Xr_train_np = pca.fit_transform(Xr_train_np)

forest.fit(Xr_train_np, yr_train) # pass training set


yr_train_np = pd.DataFrame(yr_train).to_numpy()
yr_train_np = yr_train_np.astype(np.int).flatten()



# Plotting decision regions
plot_decision_regions(Xr_train_np, yr_train_np, clf=forest, legend=2)

# Adding axes annotations
plt.xlabel('')
plt.ylabel('')
plt.title('')
plt.savefig('Multi-class Classification: Random Forest Decision Boundary')
plt.show()

# Now test the accuracy
predictions = forest.predict(pca.transform(Xr_test))

acc =  accuracy_score(predictions,yr_test)

print('Random Forest Accuracy: ', acc)

"""**Decision Boundary for Multi-class classification**"""

# Plot the decision region
import itertools    
from mlxtend.plotting import plot_decision_regions
import matplotlib.pyplot as plt
gs = gridspec.GridSpec(2, 2)

fig = plt.figure(figsize=(10,8))

labels = ['Logistic Regression', 'Support Vector Machine', 'K Nearest Neighbor', 'Decision Tree', 'Ensemble']

#for clf, lab, grd in zip ([clf1, clf2, clf3, clf4, eclf],
#                         itertools.product([0, 1], repeat=)):


# convert X_resDataFrame_std to numpy
Xr_std_np = pd.DataFrame(X_resDataframe1_df).to_numpy()
Xr_std_np = Xr_std_np.astype(np.float)

pca = PCA(n_components=2)

Xr_std_np = pca.fit_transform(Xr_std_np)

yr_np = pd.DataFrame(Y_resDataframe1).to_numpy()
yr_np = yr_np.astype(np.int).flatten()

for clf, lab, grd in zip([clf1, clf2, clf3, clf4, eclf], labels, itertools.product([0, 1], repeat=2)):
    clf.fit(Xr_std_np, Y_resDataframe1)
   
    ax = plt.subplot(gs[grd[0], grd[1]])
    fig = plot_decision_regions(X=Xr_std_np, y=yr_np, clf=clf)
   
    plt.savefig('Decision Boundaries For Multi-class Classification')
    plt.title(lab)